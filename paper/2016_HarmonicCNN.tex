% -----------------------------------------------
% Template for ISMIR Papers
% 2016 version, based on previous ISMIR templates

% Requirements :
% * 6+1 page length maximum
% * 2MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage{spconf,amsmath,cite}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{cleveref}

% Title.
% ------
\title{Exploiting Harmonics with Convolutional Neural Networks}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author}


% Three addresses
% --------------
\twoauthors
  {First Author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second Author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
%  {Third Author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%% To make customize author list in Creative Common license, uncomment and customize the next line
%  \def\authorname{First Author, Second Author, Third Author}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
%\multauthor
%{First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$} { \bfseries{Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%  $^1$ Department of Computer Science, University , Country\\
%$^2$ International Laboratories, City, Country\\
%$^3$  Company, Address\\
%{\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%}
%\def\authorname{First author, Second author, Third author, Fourth author, Fifth author, Sixth author}


%\sloppy % please retain sloppy command for improved formatting

\begin{document}

%
\maketitle
%
\begin{abstract}

% Convnets have rocked MIR
Convolutional neural networks, originally pioneered in computer vision, have shown great performance in content-based MIR across a variety of tasks.
% Operate on time-freq reps, which are not images
In vision applications, convolutional networks exploit statistical regularities in spatial locality, \emph{e.g.}, neighboring pixels.
However, in audio, these models typically operate on time-frequency representations, which exhibit different statistical properties from natural images.
% Sound is harmonic, and model assumptions may not hold.
Although time-frequency bins do exhibit local regularities, modeling only these local interactions ignores the relationships between harmonically related frequencies.
% Make input rep harmonic, does it matter?
To better adapt convolutional networks to audio, we craft two alternative input representations that aim to exploit this known behavior directly.
%and determine what effect these have on the efficacy of the learned model.
% How do we do it?
We study these approaches in a large, solo instrument classification task, and examine how model architecture influences performance.
% What do we find?
%Experimental results show that the use of harmonic convolutions leads to better generalization, faster computation time, and more compact models per unit of performance.
% Closeout
Finally, we provide an open source framework to reproduce both our results and the data used herein.

\end{abstract}


\section{Introduction}\label{sec:introduction}

% Deep learning, so hot right now... deep learning.
Many classic tasks in MIR aim to develop computational systems for modeling the human perception of music signals to achieve robust content understanding at scale, e.g. tens of millions of tracks.
In the last several years, a number of research communities have fully embraced feature learning and deep networks, which excel at modeling the kinds of complex, nonlinear behavior found in acoustic signals.
Convolutional neural networks (CNNs) are now an especially common choice for MIR tasks, finding use in timbre modelling, chord estimation, onset detection, structural segmentation, on so on.

It is worth noting that convolutional networks draw inspiration from biological vision systems \cite{Hubel1964, LeCun1998}, and common implementations have been architected with computer vision in mind.
CNNs are characterized by two features:
one, local receptive fields exploit statistical properties of natural images, i.e. neighboring pixels tend to be more correlated than distant ones;
and two, weights over these local receptive fields are shared across spatial dimensions, providing translation invariant features.

We can visualize why this is a good idea.
Here, we take the CIFAR dataset, which consists of small (32x32) images.
We use the training set of 25k images, compute a 2D auto-correlation between each image and a slightly cropped version of itself, and average over the dataset;
this is defined formally by the following.
As expected, \Cref{fig:image_corr} illustrates that neighboring pixels are better correlated than those at a distance, and there is little observable bias in either dimension.

Intuitively, time-frequency representations are not images, but much previous work overlooks this reality, applying the standard CNN formulation to audio tasks.
This raises an interesting question: how do these correlations differ from images, and can we similarly leverage this prior knowledge?
We therefore repeat the same exercise, instead drawing observations from constant-Q representations of audio.
Using the librosa toolkit, we compute CQT spectra over a number of recordings in the USPop dataset\cite{Ellis2002} and draw 25k windowed observations at random.
As before, the valid, 2D auto-correlation is performed between each windowed observation and a cropped version of itself.
Given in \Cref{fig:cqt_corr}, there are two notable distinctions.
First, consistent with intuition, the CQT representation exhibits strong correlations with harmonically related frequencies, being the octave (2:1), fifth (3:2), and major third (4:3).
Second, these correlations are not symmetrical in frequency.

In this work, we apply Harmonic Convolution Neural Networks (HCNNs) to the task of solo music instrument classification, using a newly standardized dataset referred to the Music INSTrument (MINST) dataset.
%The remainder of the paper is organized as follows:
%Section II details our approach to exploiting correlations in convolutional networks;
%Section III describes the experimental method used in this work;
%and Section IV discusses results and conclusions drawn therein.


\section{Harmonic Convolution Neural Networks}\label{sec:page_size}

% 1. convnets in time-frequency (stft, 1d)
% 2. convnets in time-logf (cqt, 1d/full)
% 3. convnets in time-logf (cqt, 2d/small)
% 4. lostanlen: octave relations
% 5. us: harmonic relations

\begin{figure}
    \includegraphics[width=\columnwidth]{figs/filter-shapes}
    \caption{A comparison of different sparsity patterns for convolutional networks on spectrogram inputs.
        \emph{Full} connectivity requires that the filter connect to all frequency bins in a temporal window around the sample frame at time $t$.
        \emph{Frequency-local} connectivity only connects a filter to near-by frequencies at time-frequency $(t, f)$.
        \emph{Octave-local} connectivity allows a filter to connect to the same pitch classes across different octaves at time-frequency $(t, f)$: that is, frequencies related to $f$ by integer powers of 2 $f' = f \cdot 2^k$.
        \emph{Harmonic-local} connectivity allows a filter to connect to harmonically related frequencies: $f' \in \{f, 2f, 3f, \dots\}$.}
\label{figs:filter-shapes}
\end{figure}

As illustrated in \Cref{figs:filter-shapes}, there are a variety of ways to model ``locality'' between time-frequency bins with a convolutional network.
Arguably the simplest method is to model the entire frequency range over a short time window.  
This method, denoted as \emph{Full} in \Cref{figs:filter-shapes} requires filter coefficients which span the entire frequency axis of the spectrogram.
While this model makes the fewest possible assumptions about independence among frequencies, it also cannot be made pitch-invariant since the filter cannot move along the frequency axis.
Consequently, it may take a large number of filters --- each with a large number of coefficients --- and thus a large amount of training data to model the relatively simply phenomenon of pitch modulation.
However, this approach has been demonstrated to be effective for high-level tasks. %TODO: cite sander, maybe pablo, eric, etc\ldots

The second approach uses smaller filters, which can be moved along the frequency axis.
For this approach to properly generate pitch-invariant features, the frequency axis of the spectrogram must be logarithmically spaced, as in the constant-Q transform (CQT).
This method, denoted as \emph{Frequency-local} in \Cref{figs:filter-shapes}, is closely related to the local receptive field filters used in computer vision applications.
Frequency-local filters can therefore capture highly localized characteristics of sound, such as attack profile or vibrato, but cannot directly capture properties derived from distant frequencies, such as timbre.
Higher-level properties can be modeled by subsequent processing layers, and this approach has been shown to be effective for intermediate-level tasks such as instrument recognition. %self-cite. TODO: eric, did you use this for chords?


Recently, Lotsanlen~\nocite{lostanlen2016} proposed a variant of the Frequency-local approach in which filters are connected to small time-frequency patches tiled across multiple octaves.
This method is depicted in \Cref{figs:filter-shapes} as \emph{octave-local}.
As the illustration shows, this connectivity scheme allows for a filter to be pitch-invariant while modeling local time-frequency statistics as well as broad spectrum properties.
Consequently, this approach --- when used in combination with the previous two methods --- demonstrated superior performance for instrument recognition, which requires both pitch-invariance and broad-spectrum models.
Note, however, that connections are only included to frequencies related by an integer power of 2 to the bottom time-frequency patch.
Octave-local design therefore disregards odd (and composite) harmonics, which can be highly informative for determining the timbre of a sound.

%Reference Lostanlen's ISMIR work.
%The facets we consider differently are...



% TODO: move this down to the next section

The width (time) of the filters should match that of MH: above I suggested 5x5, but I think 7x7 might be better since it captures +- 1 semitone from the center bin. In M2, this translates to +- 3 semitones. For MF, layer 1 will be full-height by 7.

For layer 2 in MF, the input will have shape (n[1], 1, T - 7//2); frequency information is collapsed out by valid-mode convolution, and we effectively have two-dimensional data (filter number, time). Layer 2 filters then must have shape like (1, width), and the width may as well match the width of layer 2 in MH/M2 (I suggested 9x9 above).

After that, we max-pool over time and feed into dense layer just as in the other architectures.

\begin{figure}
    \label{fig:cqt_corr}
\end{figure}
\begin{figure}
    \label{fig:image_corr}
\end{figure}


\section{Method}\label{sec:typeset_text}

\subsection{The MINST Dataset}\label{subsec:body}

% Issues with audio datasets
Training and evaluating supervised machine learning approaches on MIR tasks faces a near-universal issue of data.
% Not many open-audio datasets that can be shared.
While many researchers have built or leveraged labeled collections of audio, concerns of copyright undermine the reproducibility of results.
% Data integrity issues, many MIR tasks are subjective, raising issues of evaluation and the use of ground truth.
In many instances though, collections that do provide access to audio data suffer from questions of validity and integrity, such as GTZAN \cite{Sturm} or Ballroom \cite{Sturm}, or focus on subjective tasks which raise issues of ``ground truth''.
% Seldom large in size
Complementing these issues, large audio datasets are uncommon,
Combined, these challenges make it difficult to form robust conclusions across experiments and researchers.

Sizeable, open, and objective of datasets like MNIST \cite{}, CIFAR \cite{}, and NORB \cite{}, however, have served the computer vision community well in the last two decades, helping lead to a number of breakthroughs in the field.
It is hard to imagine, for example, where the community would be without the early role served by MNIST.
Here, we strive to develop a similarly standardized data by building on top of previous efforts in the MIR community to build collections of solo instrument samples.

Here, we combine a number of open datasets in a common manner to yield the Music Instrument dataset, dubbed MINST\footnote{\url{http://github.com/ejhumphrey/minst-dataset}}, in honor of its handwritten digit counterpart.
Note onsets are annotated semi-automatically;
a number of onset detection methods are applied, and subsequently verified or corrected manually, as needed.
These reference note onsets are then used to segment longer recordings into individual notes.
Here, each note is trimmed to a constant 1-second duration; where observed notes are shorter than this, a -65dBFS Gaussian noise signal is appended.
This results in nearly 39k note observations from 13 instrument classes, spanning 3 different corpora:
UIowa Music Instrument Samples\footnote{}, RWC\footnote{}, and Philharmonia\footnote{}.
The data used herein is made freely available online\footnote{}.


\subsection{Evaluation}



\subsection{Experimental Design}

Please include the copyright notice exactly as it appears here in the lower left-hand corner of the page.
It is set in 8pt Times.


\section{Discussion}

First level headings are in Times 10pt bold,
centered with 1 line of space above the section head, and 1/2 space below it.
For a section header immediately followed by a subsection header, the space should be merged.

\subsection{Second Level Headings}

Second level headings are in Times 10pt bold, flush left,
with 1 line of space above the section head, and 1/2 space below it.
The first letter of each significant word is capitalized.

\subsubsection{Third and Further Level Headings}

Third level headings are in Times 10pt italic, flush left,
with 1/2 line of space above the section head, and 1/2 space below it.
The first letter of each significant word is capitalized.

Using more than three levels of headings is highly discouraged.


% For bibtex users:
\bibliography{references}

\end{document}
