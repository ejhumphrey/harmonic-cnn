import copy
import logging
import numpy as np
import os
import pandas
import sklearn.metrics

import wcqtlib.common.utils as utils

logger = logging.getLogger(__name__)

DATASETS = ["rwc", "uiowa", "philharmonia"]


def concat_dataset_column(predictions_df, features_df):
    """Joins the features and predictions on the index, and
    concatenates the "dataset" field from features_df to the
    predictions_df.

    Returns
    -------
    joined_predictions : pandas.DataFrame
        predictions_df including the dataset.
    """
    return pandas.concat([
        predictions_df,
        features_df[features_df.index.isin(predictions_df.index)]['dataset']],
        axis=1)


class PredictionAnalyzer(object):
    """Worker class which cretes analysis dataframes from
    the predictions.
    """

    def __init__(self, predictions_df, features_df=None, test_set=None):
        """
        Parameters
        ----------
        predictions_df : pandas.DataFrame
            DataFrame containing predictions generated by
            evaluate.evaluate_df. Indeces from the predictions_df
            should match indeces in the features_df.

        features_df : pandas.DataFrame or None
            DataFrame pointing to the original audio, features, and
            all associated metadata.

        test_set : str in ["rwc", "uiowa", "philharmonia"] or None
            Operates only on files from the dataset specified.
            Or, if None, operates on all files.
        """
        if features_df is not None:
            self.predictions_df = concat_dataset_column(
                predictions_df, features_df)
        else:
            if test_set is not None and \
                    "dataset" not in predictions_df.columns:
                raise KeyError("You must either provide the features_df "
                               "or the dataset column in the predictions_df.")
            self.predictions_df = predictions_df

        self.set_test_set(test_set)

    def view(self, dataset):
        """Returns a copy of the analyzer pointing to the desired dataset."""
        thecopy = copy.copy(self)
        thecopy.set_test_set(dataset)
        return thecopy

    @classmethod
    def from_config(cls, config, experiment_name, model_name, test_set):
        features_path = os.path.join(
            os.path.expanduser(config["paths/extract_dir"]),
            config["dataframes/features"])
        experiment_dir = os.path.join(
            os.path.expanduser(config['paths/model_dir']),
            experiment_name)
        predictions_df_path = os.path.join(
            experiment_dir,
            config.get('experiment/predictions_format')
            .format(model_name))

        features_df = pandas.read_pickle(features_path)
        if not os.path.exists(predictions_df_path):
            raise OSError("Predictions do no not yet exist for: {}"
                          .format(predictions_df_path))
        predictions_df = pandas.read_pickle(predictions_df_path)

        return cls(predictions_df, features_df, test_set)

    def set_test_set(self, test_set):
        if test_set is not None and test_set not in DATASETS:
            raise ValueError("{} is not a valid dataset.".format(test_set))

        self.test_set = test_set
        if self.test_set is not None:
            # set up a cache to make this faster
            self._view = self.predictions_df[
                self.predictions_df["dataset"] == self.test_set]
        else:
            self._view = self.predictions_df

    def save(self, write_path):
        """
        Parameters
        ----------
        write_path : str
            Serialize this class so we can reuse it later. Path should
            be a 'pkl' file.
        """
        self.predictions_df.to_pickle(write_path)

    @classmethod
    def load(cls, read_path, test_set=None):
        """
        Parameters
        ----------
        read_path : str
            Path to deserialize the analysis from.
        """
        pred_df = pandas.read_pickle(read_path)
        return cls(pred_df, test_set=test_set)

    @property
    def y_true(self):
        return self._view["target"].tolist()

    @property
    def y_pred(self):
        return self._view["max_likelyhood"].tolist()

    @property
    def mean_loss(self):
        return self._view["mean_loss"].mean()

    @property
    def accuracy(self):
        return self.tps.sum().mean()

    @property
    def tps(self):
        return self._view["max_likelyhood"] == self._view["target"]

    @property
    def support(self):
        # You have to be careful with bincount, because it won't
        # count classes that don't have any data.
        support_array = np.zeros(len(self.classes))
        counts = np.bincount(self._view["target"])
        support_array[:len(counts)] = counts
        return support_array

    @property
    def classes(self):
        if not hasattr(self, "_classes"):
            self._classes = sorted(np.unique(self.predictions_df["target"]))

        return self._classes

    @property
    def confusion_matrix(self):
        return sklearn.metrics.confusion_matrix(self.y_true, self.y_pred)

    @property
    def classification_report(self):
        return sklearn.metrics.classification_report(self.y_true, self.y_pred)

    def class_wise_scores(self):
        """
        Return a pandas DataFrame for scores for each class,
        where the scores are the columns, and the classes are the index.
        """
        if not self._view.empty:
            precision = sklearn.metrics.precision_score(
                self.y_true, self.y_pred, labels=self.classes, average=None)
            recall = sklearn.metrics.recall_score(
                self.y_true, self.y_pred, labels=self.classes, average=None)
            f1score = sklearn.metrics.f1_score(
                self.y_true, self.y_pred, labels=self.classes, average=None)

            return pandas.DataFrame({
                "precision": precision,
                "recall": recall,
                "f1score": f1score,
                "support": self.support})
        else:
            return pandas.DataFrame(columns=[
                "precision", "recall", "f1score", "support"])

    def dataset_class_wise(self):
        print("total", self.class_wise_scores())
        print("rwc", self.view("rwc").class_wise_scores())
        print("uiowa", self.view("uiowa").class_wise_scores())
        print("phil", self.view("philharmonia").class_wise_scores())
        per_dataset_scores = pandas.concat([
            self.class_wise_scores(),
            self.view("rwc").class_wise_scores(),
            self.view("uiowa").class_wise_scores(),
            self.view("philharmonia").class_wise_scores()
        ], keys=["overall", "rwc", "uiowa", "philharmonia"])
        return per_dataset_scores

    def summary_scores(self):
        """Return summary scores over the entire dataset."""
        accuracy = sklearn.metrics.accuracy_score(
            self.y_true, self.y_pred)
        precision = sklearn.metrics.precision_score(
            self.y_true, self.y_pred, average="weighted")
        recall = sklearn.metrics.recall_score(
            self.y_true, self.y_pred, average="weighted")
        f1score = sklearn.metrics.f1_score(
            self.y_true, self.y_pred, average="weighted")
        return pandas.Series([accuracy, precision, recall, f1score],
                             index=["accuracy", "precision",
                                    "recall", "f1score"])

    def dataset_summary(self):
        per_dataset_scores = {
            "overall": self.summary_scores(),
            "rwc": self.view("rwc").summary_scores(),
            "uiowa": self.view("uiowa").summary_scores(),
            "philharmonia": self.view("philharmonia").summary_scores()
        }
        return pandas.DataFrame.from_dict(per_dataset_scores, orient="index")
